import asyncio
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from app.models.simulation import SimulationRequest
from app.services.azure_storage import AzureStorageService
from app.services.model_api import ModelAPIClient
from app.services.performance import PerformanceAnalyzer
import logging
import os

logger = logging.getLogger(__name__)

# ì „ì—­ ì‹œë®¬ë ˆì´ì…˜ ì €ì¥ì†Œ
simulations: Dict[str, dict] = {}

def load_ground_truth(file_path: str) -> Dict[str, str]:
    """Ground Truth ë§¤í•‘ íŒŒì¼ ë¡œë“œ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Ground Truth íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

class BatchSimulationProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜ í”„ë¡œì„¸ì„œ"""

    def __init__(self, batch_size: int = 10):
        self.batch_size = batch_size

    async def download_batch_images(self, azure_service: AzureStorageService,
                                    filenames: List[str]) -> List[Tuple[str, Optional[bytes]]]:
        """ë°°ì¹˜ë¡œ ì´ë¯¸ì§€ë“¤ ë‹¤ìš´ë¡œë“œ"""
        tasks = []
        for filename in filenames:
            task = self._download_single_image(azure_service, filename)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        batch_data = []
        for i, result in enumerate(results):
            filename = filenames[i]
            if isinstance(result, Exception):
                logger.error(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {filename}: {result}")
                batch_data.append((filename, None))
            else:
                batch_data.append((filename, result))

        return batch_data

    async def _download_single_image(self, azure_service: AzureStorageService,
                                     filename: str) -> Optional[bytes]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        return await azure_service.download_image(filename)

    async def predict_batch_images(self, api_client: ModelAPIClient,
                                   batch_data: List[Tuple[str, Optional[bytes]]]) -> List[Dict]:
        """ë°°ì¹˜ë¡œ ì´ë¯¸ì§€ë“¤ ì˜ˆì¸¡"""
        # ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ë“¤ë§Œ í•„í„°ë§
        valid_images = [(filename, image_data) for filename, image_data in batch_data if image_data is not None]
        failed_images = [(filename, image_data) for filename, image_data in batch_data if image_data is None]

        results = []

        # ì‹¤íŒ¨í•œ ì´ë¯¸ì§€ë“¤ ê²°ê³¼ ì¶”ê°€
        for filename, _ in failed_images:
            results.append({
                "filename": filename,
                "predicted_label": None,
                "confidence": 0.0,
                "error": "ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
            })

        # ìœ íš¨í•œ ì´ë¯¸ì§€ë“¤ ë³‘ë ¬ ì˜ˆì¸¡
        if valid_images:
            batch_results = await self._predict_with_parallel(api_client, valid_images)
            results.extend(batch_results)
        return results

    async def _predict_with_batch_api(self, api_client: ModelAPIClient,
                                      valid_images: List[Tuple[str, bytes]]) -> List[Dict]:
        """ë°°ì¹˜ APIë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
        try:
            # ëª¨ë¸ ì„œë²„ì˜ ë°°ì¹˜ API í˜¸ì¶œ
            filenames = [filename for filename, _ in valid_images]
            image_data_list = [image_data for _, image_data in valid_images]

            batch_response = await api_client.predict_batch(image_data_list, filenames)

            results = []
            for i, batch_item in enumerate(batch_response.data.results):
                if batch_item.success:
                    results.append({
                        "filename": batch_item.filename,
                        "predicted_label": batch_item.result.predicted_class,
                        "confidence": batch_item.result.confidence,
                        "error": None
                    })
                else:
                    results.append({
                        "filename": batch_item.filename,
                        "predicted_label": None,
                        "confidence": 0.0,
                        "error": batch_item.error
                    })

            return results

        except Exception as e:
            logger.error(f"ë°°ì¹˜ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # ë°°ì¹˜ API ì‹¤íŒ¨ì‹œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ í´ë°±
            return await self._predict_with_parallel(api_client, valid_images)

    async def _predict_with_parallel(self, api_client: ModelAPIClient,
                                     valid_images: List[Tuple[str, bytes]]) -> List[Dict]:
        """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
        tasks = []
        for filename, image_data in valid_images:
            task = api_client.predict(image_data, filename)
            tasks.append(task)

        predict_results = await asyncio.gather(*tasks, return_exceptions=True)

        results = []
        for i, predict_result in enumerate(predict_results):
            filename = valid_images[i][0]

            if isinstance(predict_result, Exception):
                results.append({
                    "filename": filename,
                    "predicted_label": None,
                    "confidence": 0.0,
                    "error": str(predict_result)
                })
            else:
                predicted_label, confidence, error = predict_result
                results.append({
                    "filename": filename,
                    "predicted_label": predicted_label,
                    "confidence": confidence,
                    "error": error
                })

        return results


async def run_simulation(simulation_id: str, config: SimulationRequest):
    """ë°°ì¹˜ ì²˜ë¦¬ ê°œì„ ëœ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"""
    try:
        # ìƒíƒœ ì´ˆê¸°í™”
        simulations[simulation_id].update({
            "status": "running",
            "start_time": datetime.now(),
            "total_images": 0,
            "processed_images": 0,
            "results": [],
            "current_batch": 0,
            "total_batches": 0
        })

        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        azure_service = AzureStorageService(config.azure_connection_string, config.container_name)
        await azure_service.connect()

        api_client = ModelAPIClient(config.model_api_url)
        analyzer = PerformanceAnalyzer()

        # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (batch_sizeëŠ” configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
        batch_size = getattr(config, 'batch_size', 10)
        batch_processor = BatchSimulationProcessor(batch_size=batch_size)

        # Ground Truth ë¡œë“œ
        ground_truth = load_ground_truth(config.test_mapping_path)
        if not ground_truth:
            raise Exception("Ground Truth ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")


        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        image_files = await azure_service.list_image_files(prefix=config.image_prefix)
        filtered_files = [f for f in image_files if os.path.basename(f) in ground_truth]

        if not filtered_files:
            raise Exception("ì²˜ë¦¬í•  ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

        if config.limit:
            filtered_files = filtered_files[:config.limit]

        # ë°°ì¹˜ ì •ë³´ ì—…ë°ì´íŠ¸
        total_batches = (len(filtered_files) + batch_size - 1) // batch_size
        simulations[simulation_id].update({
            "total_images": len(filtered_files),
            "total_batches": total_batches
        })

        logger.info(f"ì‹œë®¬ë ˆì´ì…˜ {simulation_id}: {len(filtered_files)}ê°œ ì´ë¯¸ì§€ë¥¼ {total_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ ì‹œì‘")

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        all_results = []

        for batch_idx in range(0, len(filtered_files), batch_size):
            batch_files = filtered_files[batch_idx:batch_idx + batch_size]
            current_batch_num = (batch_idx // batch_size) + 1

            logger.info(f"ë°°ì¹˜ {current_batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch_files)}ê°œ ì´ë¯¸ì§€)")

            try:
                # 1. ë°°ì¹˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                batch_images = await batch_processor.download_batch_images(azure_service, batch_files)

                # 2. ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰
                batch_predictions = await batch_processor.predict_batch_images(api_client, batch_images)

                # API ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                logger.info(f"ğŸ“Š API ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì‹ : {len(batch_predictions)}ê°œ")
                for i, prediction in enumerate(batch_predictions):
                    logger.info(f"ğŸ” ì˜ˆì¸¡ ê²°ê³¼ {i+1}:")
                    logger.info(f"   íŒŒì¼ëª…: {prediction['filename']}")
                    logger.info(f"   ì˜ˆì¸¡ê°’: {prediction['predicted_label']}")
                    logger.info(f"   ì‹ ë¢°ë„: {prediction['confidence']}")
                    logger.info(f"   ì˜¤ë¥˜: {prediction.get('error', 'ì—†ìŒ')}")

                # 3. ê²°ê³¼ ì²˜ë¦¬ ë° Ground Truth ë¹„êµ
                logger.info(f"ğŸ”— Ground Truth ë§¤ì¹­ ì‹œì‘")
                for prediction in batch_predictions:
                    filename = prediction["filename"]
                    predicted_label = prediction["predicted_label"]

                    # íŒŒì¼ëª…ë§Œ ì¶”ì¶œí•´ì„œ Ground Truthì™€ ë§¤ì¹­
                    filename_only = os.path.basename(filename)
                    gt_label = ground_truth.get(filename_only, "unknown")

                    # ë§¤ì¹­ ê²°ê³¼ ë¡œê¹…
                    logger.info(f"ğŸ“‹ ë§¤ì¹­: {filename_only}")
                    logger.info(f"   ì˜ˆì¸¡: {predicted_label}")
                    logger.info(f"   ì •ë‹µ: {gt_label}")
                    logger.info(f"   ì¼ì¹˜: {'âœ…' if predicted_label == gt_label else 'âŒ'}")

                    result = {
                        "filename": filename,
                        "predicted_label": predicted_label,
                        "confidence": prediction["confidence"],
                        "ground_truth": gt_label,
                        "is_correct": predicted_label == gt_label if predicted_label else False,
                        "error": prediction["error"]
                    }

                    all_results.append(result)

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                simulations[simulation_id].update({
                    "processed_images": len(all_results),
                    "current_batch": current_batch_num,
                    "results": all_results
                })

                # í˜„ì¬ê¹Œì§€ì˜ ì •í™•ë„ ê³„ì‚°
                if all_results:
                    current_metrics = analyzer.calculate_metrics(all_results)
                    simulations[simulation_id]["accuracy"] = current_metrics["accuracy"]

                    # ë°°ì¹˜ ìš”ì•½ ë¡œê·¸
                    batch_correct = sum(1 for r in all_results[-len(batch_predictions):] if r['is_correct'])
                    logger.info(f"ğŸ¯ ë°°ì¹˜ {current_batch_num} ì™„ë£Œ:")
                    logger.info(f"   ì²˜ë¦¬ëœ íŒŒì¼: {len(batch_predictions)}ê°œ")
                    logger.info(f"   ì •í™•í•œ ì˜ˆì¸¡: {batch_correct}ê°œ")
                    logger.info(f"   ë°°ì¹˜ ì •í™•ë„: {batch_correct/len(batch_predictions)*100:.1f}%")
                    logger.info(f"   ì „ì²´ ì§„í–‰ë¥ : {len(all_results)}/{len(filtered_files)} ({len(all_results)/len(filtered_files)*100:.1f}%)")
                    logger.info(f"   ëˆ„ì  ì •í™•ë„: {current_metrics['accuracy']:.4f}")

                # ë§Œì•½ API í˜¸ì¶œ ìì²´ê°€ ì‹¤íŒ¨í•œë‹¤ë©´
                if not batch_predictions:
                    logger.warning("âš ï¸  APIì—ì„œ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤!")
                elif len(batch_predictions) != len(batch_files):
                    logger.warning(f"âš ï¸  ìš”ì²­ íŒŒì¼ ìˆ˜({len(batch_files)})ì™€ ì‘ë‹µ ìˆ˜({len(batch_predictions)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤!")

                # ì˜ˆì¸¡ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ë³„ë„ í™•ì¸
                failed_predictions = [p for p in batch_predictions if p['predicted_label'] is None]
                if failed_predictions:
                    logger.warning(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ({len(failed_predictions)}ê°œ):")
                    for failed in failed_predictions:
                        logger.warning(f"   - {failed['filename']}: {failed.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

                # Ground Truth ë§¤ì¹­ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤
                unknown_gt = [r for r in all_results[-len(batch_predictions):] if r['ground_truth'] == 'unknown']
                if unknown_gt:
                    logger.warning(f"ğŸ” Ground Truth ë§¤ì¹­ ì‹¤íŒ¨ ({len(unknown_gt)}ê°œ):")
                    for unknown in unknown_gt:
                        filename_only = os.path.basename(unknown['filename'])
                        logger.warning(f"   - {filename_only}: Ground Truthì— ì—†ìŒ")


            except Exception as e:
                logger.error(f"ë°°ì¹˜ {current_batch_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ë°°ì¹˜ ì‹¤íŒ¨ì‹œì—ë„ ê³„ì† ì§„í–‰
                continue

        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        if all_results:
            final_metrics = analyzer.calculate_metrics(all_results)
            simulations[simulation_id].update({
                "status": "completed",
                "end_time": datetime.now(),
                "final_metrics": final_metrics,
                "results": all_results
            })

            processing_time = (datetime.now() - simulations[simulation_id]["start_time"]).total_seconds()
            logger.info(f"ì‹œë®¬ë ˆì´ì…˜ {simulation_id} ì™„ë£Œ - "
                        f"ì´ {len(all_results)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬, "
                        f"ìµœì¢… ì •í™•ë„: {final_metrics['accuracy']:.4f}, "
                        f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            # ë””ë²„ê¹…: Ground Truth ë§¤ì¹­ í™•ì¸
            for result in batch_predictions:
                filename_only = os.path.basename(result["filename"])
                gt_label = ground_truth.get(filename_only, "NOT_FOUND")
                logger.debug(f"ë§¤ì¹­ í™•ì¸: {filename_only} â†’ {gt_label}")
        else:
            raise Exception("ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

        await azure_service.disconnect()

    except Exception as e:
        simulations[simulation_id].update({
            "status": "failed",
            "error": str(e),
            "end_time": datetime.now()
        })
        logger.error(f"ì‹œë®¬ë ˆì´ì…˜ {simulation_id} ì‹¤íŒ¨: {e}")

        # Azure ì—°ê²° ì •ë¦¬
        try:
            await azure_service.disconnect()
        except:
            pass


# ë°°ì¹˜ í¬ê¸° ì„¤ì •ì„ ìœ„í•œ SimulationRequest í™•ì¥ì´ í•„ìš”í•œ ê²½ìš°
# models/simulation.pyì— batch_size: int = 10 í•„ë“œ ì¶”ê°€ ê¶Œì¥